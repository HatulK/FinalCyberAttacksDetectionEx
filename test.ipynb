{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-13T12:57:06.441998400Z",
     "start_time": "2024-02-13T12:56:57.832989100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: nltk in ./.local/lib/python3.10/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.10/site-packages (2.16.1)\r\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.36.2)\r\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.1.2)\r\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.1.4)\r\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (3.5.1)\r\n",
      "Collecting seaborn\r\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\r\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (1.3.2)\r\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (4.65.0)\r\n",
      "Collecting wordcloud\r\n",
      "  Downloading wordcloud-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m511.1/511.1 KB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: ipywidgets in ./.local/lib/python3.10/site-packages (8.1.1)\r\n",
      "Collecting shap\r\n",
      "  Downloading shap-0.44.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m535.7/535.7 KB\u001B[0m \u001B[31m19.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting optuna\r\n",
      "  Using cached optuna-3.5.0-py3-none-any.whl (413 kB)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.10/site-packages (from nltk) (2023.10.3)\r\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.10/site-packages (from nltk) (1.3.2)\r\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\r\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets) (0.3.7)\r\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from datasets) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in ./.local/lib/python3.10/site-packages (from datasets) (0.20.2)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.local/lib/python3.10/site-packages (from datasets) (2.31.0)\r\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from datasets) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.10/site-packages (from datasets) (6.0.1)\r\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.10/site-packages (from datasets) (0.70.15)\r\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./.local/lib/python3.10/site-packages (from datasets) (2023.10.0)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.local/lib/python3.10/site-packages (from datasets) (14.0.2)\r\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.local/lib/python3.10/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from datasets) (1.26.2)\r\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets) (3.9.1)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.local/lib/python3.10/site-packages (from transformers) (0.15.0)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.10/site-packages (from transformers) (0.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch) (1.9)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.10/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: triton==2.1.0 in ./.local/lib/python3.10/site-packages (from torch) (2.1.0)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.local/lib/python3.10/site-packages (from torch) (2.18.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.10/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.10/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.11.1)\r\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from wordcloud) (9.0.1)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in ./.local/lib/python3.10/site-packages (from ipywidgets) (4.0.9)\r\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in ./.local/lib/python3.10/site-packages (from ipywidgets) (3.0.9)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/lib/python3/dist-packages (from ipywidgets) (5.1.1)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/lib/python3/dist-packages (from ipywidgets) (7.31.1)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.local/lib/python3.10/site-packages (from ipywidgets) (0.2.0)\r\n",
      "Collecting cloudpickle\r\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\r\n",
      "Requirement already satisfied: numba in ./.local/lib/python3.10/site-packages (from shap) (0.58.1)\r\n",
      "Collecting slicer==0.0.7\r\n",
      "  Using cached slicer-0.0.7-py3-none-any.whl (14 kB)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in ./.local/lib/python3.10/site-packages (from optuna) (2.0.24)\r\n",
      "Collecting alembic>=1.5.0\r\n",
      "  Using cached alembic-1.13.1-py3-none-any.whl (233 kB)\r\n",
      "Collecting colorlog\r\n",
      "  Using cached colorlog-6.8.2-py3-none-any.whl (11 kB)\r\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (21.2.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (3.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\r\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in ./.local/lib/python3.10/site-packages (from numba->shap) (0.41.1)\r\n",
      "\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0mInstalling collected packages: wordcloud, slicer, colorlog, cloudpickle, alembic, shap, seaborn, optuna\r\n",
      "\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Ignoring invalid distribution -orch (/home/porat/.local/lib/python3.10/site-packages)\u001B[0m\u001B[33m\r\n",
      "\u001B[0mSuccessfully installed alembic-1.13.1 cloudpickle-3.0.0 colorlog-6.8.2 optuna-3.5.0 seaborn-0.13.2 shap-0.44.1 slicer-0.0.7 wordcloud-1.9.3\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "! /usr/bin/python3 -m pip install nltk datasets transformers torch pandas matplotlib seaborn scikit-learn tqdm wordcloud ipywidgets shap optuna seaborn textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fake News Detection\n",
    "\n",
    " In this notebook, we will use the `GonzaloA/fake_news` dataset to train a model to detect fake news. We will use the `transformers` library to fine-tune a BERT model on this task. We will also use the `optuna` library to perform hyperparameter optimization.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b3e6bc472171985"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import optuna"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:58:24.944101700Z",
     "start_time": "2024-02-22T15:58:20.631905800Z"
    }
   },
   "id": "7f733530f84a42b0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/porat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/porat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:58:25.208824900Z",
     "start_time": "2024-02-22T15:58:24.946585100Z"
    }
   },
   "id": "cb3b2d10841370f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load  and prepare the dataset\n",
    " We will use the `load_dataset` function from the `datasets` library to load the `GonzaloA/fake_news` dataset. We will then split the dataset into training, validation, and test sets. We will also clean the text by removing stopwords we discovered that the dataset contains a lot of stopwords, so we will remove them to reduce the noise in the data. \n",
    "  Since Bert can only handle sequences of up to 512 tokens, we will split the texts into chunks of 512 tokens. We will then tokenize the chunks using the BERT tokenizer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40b639c4dd540469"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/porat/.local/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "full_dataset = load_dataset('GonzaloA/fake_news')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:58:30.513054100Z",
     "start_time": "2024-02-22T15:58:25.210315800Z"
    }
   },
   "id": "1e4feaf5908e1eff"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "full_dataset['train'] = full_dataset['train'].remove_columns('Unnamed: 0')\n",
    "full_dataset['validation'] = full_dataset['validation'].remove_columns('Unnamed: 0')\n",
    "full_dataset['test'] = full_dataset['test'].remove_columns('Unnamed: 0')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:58:30.522987Z",
     "start_time": "2024-02-22T15:58:30.514543Z"
    }
   },
   "id": "ed1d1c63f72a27a4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "{'title': Value(dtype='string', id=None),\n 'text': Value(dtype='string', id=None),\n 'label': Value(dtype='int64', id=None)}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset['train'].info.features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:58:30.534906900Z",
     "start_time": "2024-02-22T15:58:30.526462900Z"
    }
   },
   "id": "19333fb6ae52e8a5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train dataset:\n",
      " title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Missing values in validation dataset:\n",
      " title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Missing values in test dataset:\n",
      " title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Duplicates in train dataset:\n",
      " 0\n",
      "Duplicates in validation dataset:\n",
      " 0\n",
      "Duplicates in test dataset:\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(full_dataset['train'])\n",
    "val_df = pd.DataFrame(full_dataset['validation'])\n",
    "test_df = pd.DataFrame(full_dataset['test'])\n",
    "\n",
    "print(\"Missing values in train dataset:\\n\", train_df.isnull().sum())\n",
    "print(\"Missing values in validation dataset:\\n\", val_df.isnull().sum())\n",
    "print(\"Missing values in test dataset:\\n\", test_df.isnull().sum())\n",
    "\n",
    "print(\"Duplicates in train dataset:\\n\", train_df.duplicated().sum())\n",
    "print(\"Duplicates in validation dataset:\\n\", val_df.duplicated().sum())\n",
    "print(\"Duplicates in test dataset:\\n\", test_df.duplicated().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:58:32.855338100Z",
     "start_time": "2024-02-22T15:58:30.536396800Z"
    }
   },
   "id": "4e60ff534ae2d458"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TITLE] token id:  50265\n",
      "[TEXT] token id:  50266\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "special_tokens = ['[TITLE]', '[TEXT]']\n",
    "num_added_toks = tokenizer.add_tokens(special_tokens, special_tokens=True)\n",
    "print('[TITLE] token id: ', tokenizer.convert_tokens_to_ids('[TITLE]'))\n",
    "print('[TEXT] token id: ', tokenizer.convert_tokens_to_ids('[TEXT]'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:58:33.144893900Z",
     "start_time": "2024-02-22T15:58:32.859313500Z"
    }
   },
   "id": "8d6cf63c6bcd84ae"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(full_dataset['train'])\n",
    "# Create a new column for the length of the text\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "\n",
    "# Create a new column for the number of unique words in the text\n",
    "df['unique_words'] = df['text'].apply(lambda x: len(set(x.split())))\n",
    "\n",
    "# Plot histograms for these new features\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot the histogram for text length\n",
    "df[df['label'] == 0]['text_length'].hist(ax=axes[0], bins=30, label='Real', alpha=0.5)\n",
    "df[df['label'] == 1]['text_length'].hist(ax=axes[0], bins=30, label='Fake', alpha=0.5)\n",
    "axes[0].set_title('Text Length')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot the histogram for unique words\n",
    "df[df['label'] == 0]['unique_words'].hist(ax=axes[1], bins=30, label='Real', alpha=0.5)\n",
    "df[df['label'] == 1]['unique_words'].hist(ax=axes[1], bins=30, label='Fake', alpha=0.5)\n",
    "axes[1].set_title('Unique Words')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:58:36.142426700Z",
     "start_time": "2024-02-22T15:58:33.149859800Z"
    }
   },
   "id": "fe60b0e5931feb9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd76afe5dc7e2e7"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-test for text length:\n",
      "TtestResult(statistic=5.587536768125204, pvalue=2.3277451618464682e-08, df=24351.0)\n",
      "T-test for unique words in text:\n",
      "TtestResult(statistic=7.943654351987345, pvalue=2.0474997020651297e-15, df=24351.0)\n",
      "T-test for text length between 0 and 500 words:\n",
      "TtestResult(statistic=-37.54496817362367, pvalue=4.2118900168021163e-241, df=2303.0)\n",
      "T-test for text length between 501 and 1000 words:\n",
      "TtestResult(statistic=12.860928538045084, pvalue=8.182262672921406e-37, df=2782.0)\n",
      "T-test for text length between 1001 and 1500 words:\n",
      "TtestResult(statistic=2.0203867740769406, pvalue=0.043472780716583365, df=2055.0)\n",
      "T-test for text length between 1501 and 2000 words:\n",
      "TtestResult(statistic=8.507490151306946, pvalue=2.6660095714087325e-17, df=3271.0)\n",
      "T-test for text length between 2001 and 2500 words:\n",
      "TtestResult(statistic=-9.436261057490452, pvalue=6.2638546257910646e-21, df=4134.0)\n",
      "T-test for text length between 2501 and 3000 words:\n",
      "TtestResult(statistic=7.967638343547487, pvalue=2.2382720873525268e-15, df=3156.0)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, ngrams\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Feature Engineering Functions\n",
    "def calculate_text_length(text):\n",
    "    return len(text)\n",
    "\n",
    "def calculate_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "def calculate_avg_word_length(text):\n",
    "    words = text.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "def calculate_sentiment_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def calculate_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return sum(1 for word in words if word in stop_words)\n",
    "\n",
    "# Apply the functions to both 'text' and 'title' columns\n",
    "for column in ['text', 'title']:\n",
    "    df[f'{column}_length'] = df[column].apply(calculate_text_length)\n",
    "    df[f'{column}_unique_words'] = df[column].apply(calculate_unique_words)\n",
    "    df[f'{column}_avg_word_length'] = df[column].apply(calculate_avg_word_length)\n",
    "    df[f'{column}_sentiment_polarity'] = df[column].apply(calculate_sentiment_polarity)\n",
    "    df[f'{column}_stop_words'] = df[column].apply(calculate_stop_words)\n",
    "\n",
    "# Statistical Testing for 'text' features\n",
    "real_news = df[df['label'] == 0]\n",
    "fake_news = df[df['label'] == 1]\n",
    "\n",
    "print(\"T-test for text length:\")\n",
    "print(ttest_ind(real_news['text_length'], fake_news['text_length']))\n",
    "\n",
    "print(\"T-test for unique words in text:\")\n",
    "print(ttest_ind(real_news['text_unique_words'], fake_news['text_unique_words']))\n",
    "\n",
    "# Data Segmentation for 'text' length\n",
    "text_length_segments = [(0, 500), (501, 1000), (1001, 1500), (1501, 2000), (2001, 2500), (2501, 3000)]\n",
    "\n",
    "for start, end in text_length_segments:\n",
    "    segment_real_news = real_news[(real_news['text_length'] >= start) & (real_news['text_length'] <= end)]\n",
    "    segment_fake_news = fake_news[(fake_news['text_length'] >= start) & (fake_news['text_length'] <= end)]\n",
    "    print(f\"T-test for text length between {start} and {end} words:\")\n",
    "    print(ttest_ind(segment_real_news['text_length'], segment_fake_news['text_length']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T16:08:21.899751700Z",
     "start_time": "2024-02-22T16:06:19.932978800Z"
    }
   },
   "id": "7ddf13c928313c8a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 bi-grams for real news:\n",
      "donald trump: 8710\n",
      "featured image: 4625\n",
      "hillary clinton: 3477\n",
      "white house: 3254\n",
      "united states: 3061\n",
      "twitter com: 2995\n",
      "pic twitter: 2855\n",
      "getty images: 2400\n",
      "president obama: 2064\n",
      "new york: 1948\n",
      "Top 10 bi-grams for fake news:\n",
      "united states: 7354\n",
      "donald trump: 6290\n",
      "white house: 5106\n",
      "washington reuters: 3950\n",
      "president donald: 3546\n",
      "north korea: 3331\n",
      "new york: 2995\n",
      "prime minister: 2534\n",
      "said statement: 2374\n",
      "trump said: 2159\n"
     ]
    }
   ],
   "source": [
    "def get_top_ngrams(corpus, n=None, grams=1):\n",
    "    vec = CountVectorizer(ngram_range=(grams, grams), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "# Top 10 bi-grams for real news\n",
    "real_news_bigrams = get_top_ngrams(real_news['text'], n=10, grams=2)\n",
    "print(\"Top 10 bi-grams for real news:\")\n",
    "for bigram, freq in real_news_bigrams:\n",
    "    print(f\"{bigram}: {freq}\")\n",
    "\n",
    "# Top 10 bi-grams for fake news\n",
    "fake_news_bigrams = get_top_ngrams(fake_news['text'], n=10, grams=2)\n",
    "print(\"Top 10 bi-grams for fake news:\")\n",
    "for bigram, freq in fake_news_bigrams:\n",
    "    print(f\"{bigram}: {freq}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T16:09:04.103041Z",
     "start_time": "2024-02-22T16:08:34.064714400Z"
    }
   },
   "id": "fd522c51ec6c0dd7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class ChunkedTextDataset(Dataset):\n",
    "    def __init__(self, titles, texts, labels, chunk_size=512):\n",
    "        self.inputs = []\n",
    "        self.attention_masks = []\n",
    "        self.labels = []\n",
    "\n",
    "        for title, text, label in zip(titles, texts, labels):\n",
    "            combined_text = '[TITLE] ' + title + ' [TEXT] ' + text\n",
    "            tokenized_text = tokenizer(combined_text, max_length=chunk_size, truncation=True,\n",
    "                                       padding='max_length', return_tensors='pt')\n",
    "            self.inputs.append(tokenized_text['input_ids'])\n",
    "            self.attention_masks.append(tokenized_text['attention_mask'])\n",
    "            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx].squeeze(),\n",
    "            'attention_mask': self.attention_masks[idx].squeeze(),\n",
    "            'labels': torch.tensor(self.labels[idx])\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:10:19.553881600Z",
     "start_time": "2024-02-22T15:10:19.547425300Z"
    }
   },
   "id": "b7df017fbda9bbfb"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def load_and_prepare_data(split, seed=42, dataset_percentage=1):\n",
    "    if split == \"train\":\n",
    "        reduced_dataset = full_dataset['train'].shuffle(seed=seed).select(\n",
    "            range(int(len(full_dataset['train']) * dataset_percentage)))\n",
    "    else:\n",
    "        reduced_dataset = full_dataset[split]\n",
    "    # reduced_texts = tqdm([remove_stopwords(text) for text in reduced_dataset[\"text\"]], desc=f'Cleaning {split} texts')\n",
    "    reduced_texts = reduced_dataset[\"text\"]\n",
    "    reduced_labels = reduced_dataset[\"label\"]\n",
    "    reduced_titles = reduced_dataset[\"title\"]\n",
    "    return reduced_titles, reduced_texts, reduced_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:10:19.560834900Z",
     "start_time": "2024-02-22T15:10:19.552888100Z"
    }
   },
   "id": "4d6d8474f30e6955"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Load dataset splits\n",
    "train_titles, train_texts, train_labels = load_and_prepare_data(\"train\")\n",
    "val_titles, val_texts, val_labels = load_and_prepare_data(\"validation\")\n",
    "test_titles, test_texts, test_labels = load_and_prepare_data(\"test\")\n",
    "\n",
    "train_dataset = ChunkedTextDataset(train_titles, train_texts, train_labels)\n",
    "val_dataset = ChunkedTextDataset(val_titles, val_texts, val_labels)\n",
    "test_dataset = ChunkedTextDataset(test_titles, test_texts, test_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:11:45.866162100Z",
     "start_time": "2024-02-22T15:10:19.563318Z"
    }
   },
   "id": "6e236df98419a387"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    " data exploration is an important step in any machine learning project. It allows us to understand the data and identify any issues that need to be addressed. In this section, we will explore the `GonzaloA/fake_news` dataset and perform some basic analysis to gain insights into the data.\n",
    " \n",
    " we first visualize the distribution of the labels in the training, validation, and test sets. We then create a word cloud to visualize the most common words in the dataset. We also plot the top 10 most common words in the dataset. Finally, we visualize the top 20 bigrams and trigrams in the dataset, annotated with the counts for each class (fake and real).\n",
    " ."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84b8a63f8b0d6bcb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a word cloud\n",
    "def generate_word_cloud(texts):\n",
    "    combined_text = \" \".join(texts)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\n",
    "        combined_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Show word cloud\n",
    "generate_word_cloud(train_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T12:01:37.537219200Z"
    }
   },
   "id": "958e846a98d81fe8"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def plot_top_words(data, n=10):\n",
    "    counter = Counter(\" \".join(data).split())\n",
    "    most_common = counter.most_common(n)\n",
    "    most_common_words = [word for word, freq in most_common]\n",
    "    frequencies = [freq for word, freq in most_common]\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.barplot(x=most_common_words, y=frequencies)\n",
    "    plt.title('Top 10 most common words')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T11:09:04.428248400Z",
     "start_time": "2024-02-22T11:09:04.427751700Z"
    }
   },
   "id": "25de8dafd486d02b"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def clean_text_for_visualization(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = text.translate(str.maketrans('', '', \"“”‘’—\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T11:09:04.428745100Z",
     "start_time": "2024-02-22T11:09:04.428248400Z"
    }
   },
   "id": "1dfd5d603d951c8f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1080x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_texts_for_visualization = [clean_text_for_visualization(text) for text in train_texts]\n",
    "plot_top_words(cleaned_texts_for_visualization)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T11:09:31.014753600Z",
     "start_time": "2024-02-22T11:09:04.428248400Z"
    }
   },
   "id": "378654092170a554"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def plot_annotated_ngrams_fixed(texts, labels, n=2, n_terms=20):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(texts)\n",
    "    bag_of_words = vec.transform(texts)\n",
    "    feature_names = vec.get_feature_names_out()\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "\n",
    "    # Calculate the counts for each class\n",
    "    class_counts = np.zeros((2, len(feature_names)))\n",
    "    for label, row in zip(labels, bag_of_words):\n",
    "        class_counts[label] += row.toarray()[0]\n",
    "\n",
    "    # Aggregate total counts for each n-gram and sort them\n",
    "    total_counts = sum_words.A1\n",
    "    indices = np.argsort(-total_counts)\n",
    "    top_indices = indices[:n_terms]\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    top_ngrams = feature_names[top_indices]\n",
    "    top_counts = total_counts[top_indices]\n",
    "    top_fake_counts = class_counts[0, top_indices]\n",
    "    top_real_counts = class_counts[1, top_indices]\n",
    "\n",
    "    data = {\n",
    "        'ngram': top_ngrams,\n",
    "        'frequency': top_counts,\n",
    "        'fake_count': top_fake_counts,\n",
    "        'real_count': top_real_counts\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    bars = sns.barplot(x='frequency', y='ngram', hue='ngram', data=df, palette='viridis', legend=False)\n",
    "    plt.title(f'Top {n}-grams in Dataset')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel(f'{n}-grams')\n",
    "\n",
    "    # Annotate bars with the count of fake and real labels\n",
    "    for i, bar in enumerate(bars.patches):\n",
    "        plt.text(\n",
    "            bar.get_width(),\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f'Fake: {int(df.iloc[i][\"fake_count\"])}, Real: {int(df.iloc[i][\"real_count\"])}',\n",
    "            va='center'\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T11:09:31.014753600Z",
     "start_time": "2024-02-22T11:09:31.003330500Z"
    }
   },
   "id": "5e16564669fb0d89"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_23611/2317921622.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mplot_annotated_ngrams_fixed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_texts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_terms\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m20\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_23611/532259790.py\u001B[0m in \u001B[0;36mplot_annotated_ngrams_fixed\u001B[0;34m(texts, labels, n, n_terms)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mplot_annotated_ngrams_fixed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_terms\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m20\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0mvec\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCountVectorizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mngram_range\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mbag_of_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtexts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mfeature_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvec\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_feature_names_out\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0msum_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbag_of_words\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\u001B[0m in \u001B[0;36mtransform\u001B[0;34m(self, raw_documents)\u001B[0m\n\u001B[1;32m   1432\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1433\u001B[0m         \u001B[0;31m# use the same matrix-building strategy as fit_transform\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1434\u001B[0;31m         \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_count_vocab\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mraw_documents\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfixed_vocab\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1435\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbinary\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1436\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfill\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\u001B[0m in \u001B[0;36m_count_vocab\u001B[0;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[1;32m   1274\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mraw_documents\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1275\u001B[0m             \u001B[0mfeature_counter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1276\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mfeature\u001B[0m \u001B[0;32min\u001B[0m \u001B[0manalyze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1277\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1278\u001B[0m                     \u001B[0mfeature_idx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfeature\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\u001B[0m in \u001B[0;36m_analyze\u001B[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[1;32m    115\u001B[0m                 \u001B[0mdoc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mngrams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_words\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m                 \u001B[0mdoc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mngrams\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdoc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\u001B[0m in \u001B[0;36m_word_ngrams\u001B[0;34m(self, tokens, stop_words)\u001B[0m\n\u001B[1;32m    268\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mn\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmin_n\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_n\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_original_tokens\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn_original_tokens\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 270\u001B[0;31m                     \u001B[0mtokens_append\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspace_join\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moriginal_tokens\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mn\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    271\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    272\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtokens\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "plot_annotated_ngrams_fixed(train_texts, train_labels, n=2, n_terms=20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T11:10:53.380311500Z",
     "start_time": "2024-02-22T11:10:35.109480700Z"
    }
   },
   "id": "b749a33cff5518b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n",
    "\n",
    " We will use the `transformers` library to fine-tune a BERT model on the fake news detection task. We will use the `Trainer` class to handle the training loop and the `TrainingArguments` class to define the training configuration. We will also define a function to compute the evaluation metrics for the task. We will use the `precision`, `recall`, and `F1 score` as the evaluation metrics. We will then use the `optuna` library to perform hyperparameter optimization to find the best hyperparameters for the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f75e233452e3dbeb"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU cuda\n"
     ]
    }
   ],
   "source": [
    "# check for cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\",device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:11:45.909868800Z",
     "start_time": "2024-02-22T15:11:45.909372Z"
    }
   },
   "id": "baccea00e333709d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    eval_predictions, eval_labels = eval_pred\n",
    "    eval_predictions = np.argmax(eval_predictions, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(eval_labels, eval_predictions, average='binary')\n",
    "    acc = accuracy_score(eval_labels, eval_predictions)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:11:45.909868800Z",
     "start_time": "2024-02-22T15:11:45.909868800Z"
    }
   },
   "id": "25f4d17014cd8f38"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters for the trial\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 1, 10)\n",
    "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16, 32])\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 500)\n",
    "\n",
    "    # Create the model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, device_map=device)\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        disable_tqdm=False,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "    )\n",
    "\n",
    "    # Create the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    # Return the metric to be optimized\n",
    "    return metrics['eval_f1']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:28:35.780004400Z",
     "start_time": "2024-02-22T13:28:35.779508100Z"
    }
   },
   "id": "c5c7f330fd8e5fe1"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-22 15:35:43,524] A new study created in memory with name: no-name-163bd3a0-6e3c-4f48-97e8-d1a304725750\n",
      "[W 2024-02-22 15:35:43,850] Trial 0 failed with parameters: {'learning_rate': 1.1408163819119355e-05, 'num_train_epochs': 5, 'per_device_train_batch_size': 16, 'warmup_steps': 136} because of the following error: RuntimeError('CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/porat/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_26464/2665237945.py\", line 9, in objective\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, device_map=device)\n",
      "  File \"/home/porat/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/home/porat/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3706, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "  File \"/home/porat/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3952, in _load_pretrained_model\n",
      "    set_module_tensor_to_device(model, key, \"cpu\", torch.empty(*param.size(), dtype=target_dtype))\n",
      "  File \"/home/porat/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 398, in set_module_tensor_to_device\n",
      "    torch.cuda.empty_cache()\n",
      "  File \"/home/porat/.local/lib/python3.10/site-packages/torch/cuda/memory.py\", line 159, in empty_cache\n",
      "    torch._C._cuda_emptyCache()\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "[W 2024-02-22 15:35:43,856] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_26464/1744117078.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Create the study and run the optimization\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mstudy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moptuna\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_study\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdirection\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"maximize\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mstudy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobjective\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_trials\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Best trial: {study.best_trial.number}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Best value (F1 score): {study.best_value}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/optuna/study/study.py\u001B[0m in \u001B[0;36moptimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    449\u001B[0m                 \u001B[0mIf\u001B[0m \u001B[0mnested\u001B[0m \u001B[0minvocation\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthis\u001B[0m \u001B[0mmethod\u001B[0m \u001B[0moccurs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    450\u001B[0m         \"\"\"\n\u001B[0;32m--> 451\u001B[0;31m         _optimize(\n\u001B[0m\u001B[1;32m    452\u001B[0m             \u001B[0mstudy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    453\u001B[0m             \u001B[0mfunc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001B[0m in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     64\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mn_jobs\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 66\u001B[0;31m             _optimize_sequential(\n\u001B[0m\u001B[1;32m     67\u001B[0m                 \u001B[0mstudy\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m                 \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001B[0m in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 163\u001B[0;31m             \u001B[0mfrozen_trial\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_run_trial\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstudy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcatch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    164\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m             \u001B[0;31m# The following line mitigates memory problems that can be occurred in some\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001B[0m in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    249\u001B[0m         \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc_err\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcatch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m     ):\n\u001B[0;32m--> 251\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mfunc_err\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    252\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mfrozen_trial\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    253\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/optuna/study/_optimize.py\u001B[0m in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    198\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mget_heartbeat_thread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_trial_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstudy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_storage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 200\u001B[0;31m             \u001B[0mvalue_or_values\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrial\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    201\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mexceptions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTrialPruned\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;31m# TODO(mamu): Handle multi-objective cases.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_26464/2665237945.py\u001B[0m in \u001B[0;36mobjective\u001B[0;34m(trial)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;31m# Create the model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAutoModelForSequenceClassification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice_map\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     \u001B[0;31m# Define the training arguments\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     training_args = TrainingArguments(\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    564\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_model_mapping\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    565\u001B[0m             \u001B[0mmodel_class\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_model_class\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_model_mapping\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 566\u001B[0;31m             return model_class.from_pretrained(\n\u001B[0m\u001B[1;32m    567\u001B[0m                 \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mhub_kwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    568\u001B[0m             )\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3704\u001B[0m                 \u001B[0moffload_index\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3705\u001B[0m                 \u001B[0merror_msgs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3706\u001B[0;31m             \u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_load_pretrained_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3707\u001B[0m                 \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3708\u001B[0m                 \u001B[0mstate_dict\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001B[0m in \u001B[0;36m_load_pretrained_model\u001B[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001B[0m\n\u001B[1;32m   3950\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mparam\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"meta\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3951\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mis_quantized\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3952\u001B[0;31m                         \u001B[0mset_module_tensor_to_device\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"cpu\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mparam\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtarget_dtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3953\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3954\u001B[0m                         set_module_quantized_tensor_to_device(\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py\u001B[0m in \u001B[0;36mset_module_tensor_to_device\u001B[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001B[0m\n\u001B[1;32m    396\u001B[0m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnpu\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mempty_cache\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    397\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 398\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mempty_cache\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    399\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    400\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/cuda/memory.py\u001B[0m in \u001B[0;36mempty_cache\u001B[0;34m()\u001B[0m\n\u001B[1;32m    157\u001B[0m     \"\"\"\n\u001B[1;32m    158\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mis_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 159\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_cuda_emptyCache\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Create the study and run the optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(f\"Best trial: {study.best_trial.number}\")\n",
    "print(f\"Best value (F1 score): {study.best_value}\")\n",
    "print(f\"Best hyperparameters: {study.best_params}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:35:42.198539400Z",
     "start_time": "2024-02-22T13:35:41.824053300Z"
    }
   },
   "id": "27b78e94d0f6ea9a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Embedding(50267, 768)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, device_map=device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:11:46.567951500Z",
     "start_time": "2024-02-22T15:11:45.909868800Z"
    }
   },
   "id": "d22c52bc6c67e643"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./API/results',\n",
    "    logging_dir='./API/logs',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-05,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:11:46.836647900Z",
     "start_time": "2024-02-22T15:11:46.567454900Z"
    }
   },
   "id": "6e2e5888968d361e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mchen24201\u001B[0m (\u001B[33mapi-project\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/porat/wandb/run-20240222_171150-m6jj2tar</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/api-project/huggingface/runs/m6jj2tar' target=\"_blank\">festive-rocket-12</a></strong> to <a href='https://wandb.ai/api-project/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/api-project/huggingface' target=\"_blank\">https://wandb.ai/api-project/huggingface</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/api-project/huggingface/runs/m6jj2tar' target=\"_blank\">https://wandb.ai/api-project/huggingface/runs/m6jj2tar</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 10.75 GiB of which 84.88 MiB is free. Process 33503 has 6.54 GiB memory in use. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.91 GiB is allocated by PyTorch, and 20.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_33881/395988435.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Training\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mhistory\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1535\u001B[0m                 \u001B[0mhf_hub_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_progress_bars\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1536\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1537\u001B[0;31m             return inner_training_loop(\n\u001B[0m\u001B[1;32m   1538\u001B[0m                 \u001B[0margs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1539\u001B[0m                 \u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mresume_from_checkpoint\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36m_inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1852\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1853\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maccelerator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maccumulate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1854\u001B[0;31m                     \u001B[0mtr_loss_step\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1855\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1856\u001B[0m                 if (\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtraining_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2733\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2734\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_loss_context_manager\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2735\u001B[0;31m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_loss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2736\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2737\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_gpu\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mcompute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2756\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2757\u001B[0m             \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2758\u001B[0;31m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2759\u001B[0m         \u001B[0;31m# Save past state if it exists\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2760\u001B[0m         \u001B[0;31m# TODO: this needs to be fixed and made cleaner later.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1517\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1518\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1519\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1525\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1528\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mreturn_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreturn_dict\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mreturn_dict\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muse_return_dict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1197\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1198\u001B[0;31m         outputs = self.roberta(\n\u001B[0m\u001B[1;32m   1199\u001B[0m             \u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1200\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1517\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1518\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1519\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1525\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1528\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    833\u001B[0m             \u001B[0mpast_key_values_length\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpast_key_values_length\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    834\u001B[0m         )\n\u001B[0;32m--> 835\u001B[0;31m         encoder_outputs = self.encoder(\n\u001B[0m\u001B[1;32m    836\u001B[0m             \u001B[0membedding_output\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    837\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mextended_attention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1517\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1518\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1519\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1525\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1528\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    522\u001B[0m                 )\n\u001B[1;32m    523\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 524\u001B[0;31m                 layer_outputs = layer_module(\n\u001B[0m\u001B[1;32m    525\u001B[0m                     \u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    526\u001B[0m                     \u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1517\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1518\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1519\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1525\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1528\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    411\u001B[0m         \u001B[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    412\u001B[0m         \u001B[0mself_attn_past_key_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpast_key_value\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mpast_key_value\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 413\u001B[0;31m         self_attention_outputs = self.attention(\n\u001B[0m\u001B[1;32m    414\u001B[0m             \u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    415\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1517\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1518\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1519\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1525\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1528\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    338\u001B[0m         \u001B[0moutput_attentions\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mbool\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    339\u001B[0m     ) -> Tuple[torch.Tensor]:\n\u001B[0;32m--> 340\u001B[0;31m         self_outputs = self.self(\n\u001B[0m\u001B[1;32m    341\u001B[0m             \u001B[0mhidden_states\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    342\u001B[0m             \u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1517\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1518\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1519\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1520\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1525\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1526\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1528\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1529\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    258\u001B[0m                 \u001B[0mattention_scores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mattention_scores\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mrelative_position_scores_query\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mrelative_position_scores_key\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    259\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 260\u001B[0;31m         \u001B[0mattention_scores\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mattention_scores\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mattention_head_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    261\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mattention_mask\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    262\u001B[0m             \u001B[0;31m# Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 10.75 GiB of which 84.88 MiB is free. Process 33503 has 6.54 GiB memory in use. Including non-PyTorch memory, this process has 4.11 GiB memory in use. Of the allocated memory 3.91 GiB is allocated by PyTorch, and 20.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T15:11:50.570584900Z",
     "start_time": "2024-02-22T15:11:47.058161100Z"
    }
   },
   "id": "2a2152af67f6f3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation\n",
    "\n",
    " We will evaluate the trained model on the test set. We will use the `evaluate` method of the `Trainer` class to compute the evaluation metrics on the test set. We will also use the `predict` method to generate predictions on the test set and compute the confusion matrix to visualize the model's performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afeaa38b55b2e82c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T15:11:50.570088100Z"
    }
   },
   "id": "ade3d900c0ada213"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x504 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:36:23.416874100Z",
     "start_time": "2024-02-22T12:35:31.527169Z"
    }
   },
   "id": "dc77935426baa9ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    " In this notebook, we used the `transformers` library to fine-tune a BERT model on the fake news detection task. We used the `optuna` library to perform hyperparameter optimization to find the best hyperparameters for the model. We then evaluated the trained model on the test set and visualized its performance using a confusion matrix. The model achieved an accuracy of 0.98 on the test set, demonstrating its effectiveness in detecting fake news."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73f7ae83f99c9f38"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "('model\\\\tokenizer_config.json',\n 'model\\\\special_tokens_map.json',\n 'model\\\\vocab.txt',\n 'model\\\\added_tokens.json',\n 'model\\\\tokenizer.json')"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T14:08:59.045104700Z",
     "start_time": "2024-02-16T14:08:58.656143800Z"
    }
   },
   "id": "bef5ca9e3ca6b190"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a506ab39d5772080"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
